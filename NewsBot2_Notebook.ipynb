{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N48aanOQGGlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gahu0tYjGIb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üì∞ NewsBot Intelligence Project\n",
        "# Author: [Christen Robinson]\n",
        "# Description: A text classification system that predicts the category of news articles\n",
        "# using TF-IDF feature extraction and Logistic Regression.\n",
        "# Environment: Google Colab\n",
        "# Dependencies: pandas, scikit-learn, numpy, beautifulsoup4, joblib\n"
      ],
      "metadata": {
        "id": "-zXXCIfL0Q0R"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn numpy beautifulsoup4 joblib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn2lPLRC0h30",
        "outputId": "d8160c94-6057-4ee6-c013-a966a735ea56"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üìÇ Please upload your news dataset CSV file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the first uploaded file\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f\"‚úÖ Uploaded file: {filename}\")\n",
        "\n",
        "# Try multiple read methods to avoid ParserError\n",
        "try:\n",
        "    df = pd.read_csv(io.BytesIO(uploaded[filename]), on_bad_lines='skip')\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Default read_csv failed: {e}\")\n",
        "    print(\"üîÑ Trying with a different separator...\")\n",
        "    try:\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[filename]), sep=';', on_bad_lines='skip')\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è Still failed. Trying pipe-delimited read...\")\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[filename]), sep='|', on_bad_lines='skip')\n",
        "\n",
        "print(\"\\n‚úÖ Data loaded successfully (some bad lines skipped if needed).\")\n",
        "print(f\"üßæ Rows: {len(df)}, Columns: {list(df.columns)}\")\n",
        "\n",
        "# --- OPTIONAL: Clean up strange structures like HTML or nested JSON ---\n",
        "def strip_html(text):\n",
        "    import re\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'<[^>]+>', '', text)\n",
        "    return text\n",
        "\n",
        "df = df.applymap(strip_html)\n",
        "\n",
        "print(\"\\nüìä Data preview:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "cxo0oCEvkxTl",
        "outputId": "d722b710-9db1-4ae9-8d78-843bfc5d10cd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Please upload your news dataset CSV file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d7468d04-90a2-455e-9427-5471261dd0aa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d7468d04-90a2-455e-9427-5471261dd0aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving news_data_clean.csv to news_data_clean (2).csv\n",
            "‚úÖ Uploaded file: news_data_clean (2).csv\n",
            "\n",
            "‚úÖ Data loaded successfully (some bad lines skipped if needed).\n",
            "üßæ Rows: 10, Columns: ['text', 'label']\n",
            "\n",
            "üìä Data preview:\n",
            "                                                text        label\n",
            "0  Government passes new healthcare reform bill i...     Politics\n",
            "1  Tech giant releases latest smartphone with AI ...   Technology\n",
            "2  Local football team wins championship after th...       Sports\n",
            "3  Stock markets rise as investors gain confidenc...     Business\n",
            "4  Scientists discover new species of frog in Ama...  Environment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3925217810.py:34: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(strip_html)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# üßπ DATA RESCUE & CLEAN REBUILD\n",
        "# ==============================\n",
        "import pandas as pd\n",
        "import re\n",
        "import io\n",
        "from bs4 import BeautifulSoup  # for HTML cleanup\n",
        "\n",
        "# In case df failed to load or looks malformed\n",
        "if 'df' not in locals() or len(df.columns) <= 1:\n",
        "    print(\"‚ö†Ô∏è Existing DataFrame seems malformed. Attempting auto-repair...\")\n",
        "\n",
        "    try:\n",
        "        raw_text = uploaded[filename].decode('utf-8', errors='ignore')\n",
        "    except Exception:\n",
        "        raw_text = str(uploaded[filename])\n",
        "\n",
        "    # Try to detect JSON-like structures\n",
        "    if '{' in raw_text and '}' in raw_text and ',' in raw_text:\n",
        "        print(\"üß© Detected JSON-like structure.\")\n",
        "        try:\n",
        "            import json\n",
        "            data = [json.loads(line) for line in raw_text.splitlines() if line.strip().startswith('{')]\n",
        "            df = pd.DataFrame(data)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è JSON parsing failed: {e}\")\n",
        "            df = pd.DataFrame({'raw': raw_text.splitlines()})\n",
        "    else:\n",
        "        print(\"üìÑ Treating file as plain text.\")\n",
        "        df = pd.DataFrame({'raw': raw_text.splitlines()})\n",
        "\n",
        "else:\n",
        "    print(\"‚úÖ Base DataFrame already loaded successfully.\")\n",
        "\n",
        "# --- STEP 2: Try to extract usable text and label columns ---\n",
        "def strip_html(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "text_col = None\n",
        "label_col = None\n",
        "\n",
        "# Common guesses for text or label columns\n",
        "possible_text_cols = ['text','content','body','headline','article','message','news']\n",
        "possible_label_cols = ['label','category','class','topic','section','type']\n",
        "\n",
        "for c in df.columns:\n",
        "    c_low = c.lower()\n",
        "    if text_col is None and any(k in c_low for k in possible_text_cols):\n",
        "        text_col = c\n",
        "    if label_col is None and any(k in c_low for k in possible_label_cols):\n",
        "        label_col = c\n",
        "\n",
        "# If we still don't find columns, rebuild manually\n",
        "if text_col is None:\n",
        "    print(\"‚ö†Ô∏è No clear text column found ‚Äî extracting text from entire dataset.\")\n",
        "    df['text'] = df.apply(lambda row: strip_html(\" \".join(map(str, row.values))), axis=1)\n",
        "else:\n",
        "    df['text'] = df[text_col].astype(str).apply(strip_html)\n",
        "\n",
        "if label_col is None:\n",
        "    df['label'] = 'unknown'\n",
        "else:\n",
        "    df['label'] = df[label_col].astype(str)\n",
        "\n",
        "# --- STEP 3: Drop duplicates and empties ---\n",
        "df = df[['text','label']].dropna(subset=['text']).drop_duplicates().reset_index(drop=True)\n",
        "print(f\"‚úÖ Rebuilt dataset with {len(df)} rows and columns: {df.columns.tolist()}\")\n",
        "\n",
        "# --- STEP 4: Save cleaned version ---\n",
        "df.to_csv(\"clean_news_data.csv\", index=False)\n",
        "print(\"üíæ Saved clean dataset as 'clean_news_data.csv'\")\n",
        "\n",
        "# --- STEP 5: Preview ---\n",
        "print(\"\\nüìä Clean sample:\")\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHjs1ZklmQAc",
        "outputId": "e20befce-a748-45d9-aa9d-a4abf64c3e5c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Base DataFrame already loaded successfully.\n",
            "‚úÖ Rebuilt dataset with 10 rows and columns: ['text', 'label']\n",
            "üíæ Saved clean dataset as 'clean_news_data.csv'\n",
            "\n",
            "üìä Clean sample:\n",
            "                                                text        label\n",
            "0  Government passes new healthcare reform bill i...     Politics\n",
            "1  Tech giant releases latest smartphone with AI ...   Technology\n",
            "2  Local football team wins championship after th...       Sports\n",
            "3  Stock markets rise as investors gain confidenc...     Business\n",
            "4  Scientists discover new species of frog in Ama...  Environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 scikit-learn pandas numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFVjFwehnIUv",
        "outputId": "ea94800b-c636-4618-8dbb-7ea59be65c90"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Reload cleaned dataset to ensure consistency\n",
        "# Assuming 'df' is already loaded and cleaned from previous steps\n",
        "# df = pd.read_csv(\"clean_news_data.csv\") # Commented out as df is likely already loaded\n",
        "\n",
        "# Normalize labels\n",
        "df['label'] = df['label'].str.lower().str.strip()\n",
        "\n",
        "# Drop empty entries\n",
        "df = df[df['text'].notna() & df['label'].notna()]\n",
        "df = df[df['text'].str.strip() != \"\"]\n",
        "\n",
        "print(f\"‚úÖ Using {len(df)} cleaned rows for training.\")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df['label'], test_size=0.2, random_state=42 # Removed stratify=df['label']\n",
        ")\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"‚úÖ TF-IDF vectorization complete: {X_train_tfidf.shape[1]} features\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNtoO3WPnXzJ",
        "outputId": "01fc3240-6301-4f3e-ff39-82520d4496e3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using 10 cleaned rows for training.\n",
            "‚úÖ TF-IDF vectorization complete: 49 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Check for sufficient classes\n",
        "if len(y_train.unique()) < 2:\n",
        "    print(\"‚ö†Ô∏è WARNING: Only one unique category found in your dataset.\")\n",
        "    print(f\"üõë Skipping model training. Please provide a dataset with at least 2 categories for classification.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Training Logistic Regression model with {len(y_train.unique())} categories...\")\n",
        "    # Train model\n",
        "    model = LogisticRegression(max_iter=300)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    print(\"\\nüìä Model Evaluation Results:\")\n",
        "    print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FOyFFm1noZe",
        "outputId": "13168723-2beb-4476-c3b0-1c055ed42ea8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training Logistic Regression model with 6 categories...\n",
            "\n",
            "üìä Model Evaluation Results:\n",
            "Accuracy: 0.0\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    business       0.00      0.00      0.00       0.0\n",
            "    politics       0.00      0.00      0.00       0.0\n",
            "  technology       0.00      0.00      0.00       2.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(model, \"newsbot_model.pkl\")\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
        "\n",
        "print(\"üíæ Model and vectorizer saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81ubz-eQpB0Q",
        "outputId": "bb8650ea-4c84-4ed3-9d61-4c1e5d2f70fe"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Model and vectorizer saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÆ NewsBot Predictor Function\n",
        "def predict_category(text):\n",
        "    cleaned_text = strip_html(text)\n",
        "    vec = vectorizer.transform([cleaned_text])\n",
        "    prediction = model.predict(vec)[0]\n",
        "    return prediction\n",
        "\n",
        "# Try it!\n",
        "sample = input(\"üì∞ Enter a news headline or paragraph: \")\n",
        "print(f\"ü§ñ Predicted Category: {predict_category(sample)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1HAQerqpJI9",
        "outputId": "47a4165d-466c-48b2-8076-11a25247a4bf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì∞ Enter a news headline or paragraph: Local team wins championship after dramatic final match\n",
            "ü§ñ Predicted Category: sports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    \"The stock market saw record highs today as tech shares rose.\",\n",
        "    \"The local team clinched victory in the championship finals.\",\n",
        "    \"New climate policy aims to reduce emissions by 2030.\",\n",
        "    \"Scientists discover new exoplanet capable of supporting life.\"\n",
        "]\n",
        "\n",
        "for text in examples:\n",
        "    print(f\"üì∞ {text}\")\n",
        "    print(f\"ü§ñ Predicted Category: {predict_category(text)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcRp1FO_qdG8",
        "outputId": "12a20f2b-7beb-487e-8d76-55f6c4f922ad"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì∞ The stock market saw record highs today as tech shares rose.\n",
            "ü§ñ Predicted Category: business\n",
            "\n",
            "üì∞ The local team clinched victory in the championship finals.\n",
            "ü§ñ Predicted Category: business\n",
            "\n",
            "üì∞ New climate policy aims to reduce emissions by 2030.\n",
            "ü§ñ Predicted Category: politics\n",
            "\n",
            "üì∞ Scientists discover new exoplanet capable of supporting life.\n",
            "ü§ñ Predicted Category: politics\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83b83ea0",
        "outputId": "a7f19b80-dcff-48f4-e701-eff4ae30c947"
      },
      "source": [
        "import joblib\n",
        "\n",
        "try:\n",
        "    # Load the saved model and vectorizer\n",
        "    model = joblib.load(\"newsbot_model.pkl\")\n",
        "    vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "    print(\"‚úÖ Model and vectorizer loaded successfully!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Model or vectorizer files not found.\")\n",
        "    print(\"Please ensure you have successfully run the cell to save the model and vectorizer.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model or vectorizer: {e}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model and vectorizer loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "b5476777",
        "outputId": "5b9814c6-0a00-4bdf-a7ba-4008be2ef5a0"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üìÇ Please upload your news dataset CSV file with 'text' and 'category' or 'label' columns...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"‚ùå No file uploaded. Please upload your CSV file to proceed.\")\n",
        "else:\n",
        "    # Get the first uploaded file\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"‚úÖ Uploaded file: {filename}\")\n",
        "\n",
        "    try:\n",
        "        # Attempt to read the CSV with common separators and error handling\n",
        "        df = pd.read_csv(io.BytesIO(uploaded[filename]), sep=',', on_bad_lines='skip')\n",
        "        print(\"\\n‚úÖ Data loaded successfully (some bad lines skipped if needed).\")\n",
        "        print(f\"üßæ Rows: {len(df)}, Columns: {list(df.columns)}\")\n",
        "\n",
        "        # Check for text and label columns\n",
        "        text_col = None\n",
        "        label_col = None\n",
        "\n",
        "        possible_text_cols = ['text', 'content', 'body', 'headline', 'article', 'message', 'news']\n",
        "        possible_label_cols = ['label', 'category', 'class', 'topic', 'section', 'type']\n",
        "\n",
        "        for col in df.columns:\n",
        "            if text_col is None and col.lower() in possible_text_cols:\n",
        "                text_col = col\n",
        "            if label_col is None and col.lower() in possible_label_cols:\n",
        "                label_col = col\n",
        "\n",
        "        if text_col and label_col:\n",
        "            # Rename columns to 'text' and 'label' for consistency with the notebook\n",
        "            if text_col != 'text':\n",
        "                df.rename(columns={text_col: 'text'}, inplace=True)\n",
        "            if label_col != 'label':\n",
        "                df.rename(columns={label_col: 'label'}, inplace=True)\n",
        "\n",
        "            # Check for multiple categories\n",
        "            if len(df['label'].unique()) < 2:\n",
        "                print(\"‚ö†Ô∏è WARNING: The 'label' column contains only one unique category.\")\n",
        "                print(\"üõë Please provide a dataset with at least 2 categories for classification.\")\n",
        "                # Optionally clear df or set a flag to prevent further processing\n",
        "                df = pd.DataFrame() # Clear the DataFrame if only one category\n",
        "            else:\n",
        "                print(f\"‚úÖ Found 'text' and 'label' columns with {len(df['label'].unique())} categories.\")\n",
        "                print(\"\\nüìä Data preview:\")\n",
        "                display(df.head())\n",
        "\n",
        "        elif text_col is None and label_col is None:\n",
        "             print(\"‚ùå ERROR: Could not find a column for text content and a column for labels.\")\n",
        "             print(\"Please ensure your CSV has columns named 'text' and 'label' (or similar, like 'content', 'category').\")\n",
        "             df = pd.DataFrame() # Clear the DataFrame if columns not found\n",
        "        elif text_col is None:\n",
        "             print(\"‚ùå ERROR: Could not find a column for text content.\")\n",
        "             print(\"Please ensure your CSV has a column named 'text' (or similar, like 'content').\")\n",
        "             df = pd.DataFrame() # Clear the DataFrame if text column not found\n",
        "        elif label_col is None:\n",
        "             print(\"‚ùå ERROR: Could not find a column for labels.\")\n",
        "             print(\"Please ensure your CSV has a column named 'label' (or similar, like 'category').\")\n",
        "             df = pd.DataFrame() # Clear the DataFrame if label column not found\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading data: {e}\")\n",
        "        print(\"Please check your CSV file format and try again.\")\n",
        "        df = pd.DataFrame() # Clear the DataFrame in case of any other loading error"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Please upload your news dataset CSV file with 'text' and 'category' or 'label' columns...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a1277389-55c9-4348-b3ae-455fae6f6eef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a1277389-55c9-4348-b3ae-455fae6f6eef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving news_data_clean.csv to news_data_clean (3).csv\n",
            "‚úÖ Uploaded file: news_data_clean (3).csv\n",
            "\n",
            "‚úÖ Data loaded successfully (some bad lines skipped if needed).\n",
            "üßæ Rows: 10, Columns: ['text', 'label']\n",
            "‚úÖ Found 'text' and 'label' columns with 7 categories.\n",
            "\n",
            "üìä Data preview:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                text        label\n",
              "0  Government passes new healthcare reform bill i...     Politics\n",
              "1  Tech giant releases latest smartphone with AI ...   Technology\n",
              "2  Local football team wins championship after th...       Sports\n",
              "3  Stock markets rise as investors gain confidenc...     Business\n",
              "4  Scientists discover new species of frog in Ama...  Environment"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37da1055-508a-4038-9797-0192ce2fd3e8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Government passes new healthcare reform bill i...</td>\n",
              "      <td>Politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tech giant releases latest smartphone with AI ...</td>\n",
              "      <td>Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Local football team wins championship after th...</td>\n",
              "      <td>Sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stock markets rise as investors gain confidenc...</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Scientists discover new species of frog in Ama...</td>\n",
              "      <td>Environment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37da1055-508a-4038-9797-0192ce2fd3e8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-37da1055-508a-4038-9797-0192ce2fd3e8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-37da1055-508a-4038-9797-0192ce2fd3e8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5bd6f8d7-6c67-4d8b-8a59-1b7cd24242ec\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5bd6f8d7-6c67-4d8b-8a59-1b7cd24242ec')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5bd6f8d7-6c67-4d8b-8a59-1b7cd24242ec button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"        df = pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Tech giant releases latest smartphone with AI features\",\n          \"Scientists discover new species of frog in Amazon rainforest\",\n          \"Local football team wins championship after thrilling final\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Technology\",\n          \"Environment\",\n          \"Sports\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7kz1gU9E2m_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ NewsBot Intelligence training complete!\")\n",
        "print(\"Model accuracy and classification report are displayed above.\")\n",
        "print(\"You can now test predictions or export the model for deployment.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMAuze4m0uQC",
        "outputId": "c07501ca-2ce4-4753-b876-757a6b7f1893"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ NewsBot Intelligence training complete!\n",
            "Model accuracy and classification report are displayed above.\n",
            "You can now test predictions or export the model for deployment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjexMmUK1Z-C"
      },
      "source": [
        "# ü§ñ NewsBot 2.0 Final Project - Student Guidance Notebook## üéØ Your Mission: Build an Advanced NLP Intelligence SystemWelcome to your final project! This notebook will guide you through building NewsBot 2.0 - a sophisticated news analysis platform that demonstrates everything you've learned in this course.### üöÄ What You're BuildingYou're creating a **production-ready news intelligence system** that can:- **Analyze** news articles with advanced NLP techniques- **Discover** hidden topics and trends in large text collections- **Understand** multiple languages and cultural contexts  - **Converse** with users through natural language queries- **Generate** insights and summaries automatically### üìö Skills You'll DemonstrateThis project integrates **ALL course modules**:- **Modules 1-2**: Advanced text preprocessing and feature engineering- **Modules 3-4**: Enhanced classification and linguistic analysis- **Modules 5-6**: Syntax parsing and semantic understanding- **Modules 7-8**: Multi-class classification and entity recognition- **Module 9**: Topic modeling and unsupervised learning- **Module 10**: Neural networks and language models- **Module 11**: Machine translation and multilingual processing- **Module 12**: Conversational AI and natural language understanding---## üó∫Ô∏è Project RoadmapThis notebook is organized into **7 major sections** that mirror your final system architecture:1. **üèóÔ∏è Project Setup & Architecture Planning**2. **üìä Advanced Content Analysis Engine** 3. **üß† Language Understanding & Generation**4. **üåç Multilingual Intelligence**5. **üí¨ Conversational Interface**6. **üîß System Integration & Testing**7. **üìà Evaluation & Documentation**Each section provides:- **Clear objectives** and success criteria- **Implementation hints** and architectural guidance- **Code templates** with TODO sections for you to complete- **Testing strategies** to validate your work- **Reflection questions** to deepen your understanding---## ‚ö†Ô∏è Important Notes### üéØ Learning Goals- **Understand** how advanced NLP systems work in production- **Implement** sophisticated text analysis pipelines- **Integrate** multiple NLP techniques into cohesive workflows- **Evaluate** system performance using appropriate metrics- **Communicate** technical concepts to business stakeholders### üö´ What This Notebook Won't Do- **Give you the answers** - you need to implement the logic- **Write your code** - you'll build everything from scratch- **Make decisions** - you'll choose the best approaches for your use case### ‚úÖ What This Notebook Will Do- **Guide your thinking** with structured questions and prompts- **Provide templates** and architectural patterns- **Suggest resources** and implementation strategies- **Help you organize** your work effectively- **Connect concepts** from different course modulesLet's begin building your NewsBot 2.0! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5KrNG8a1Z-H"
      },
      "source": [
        "## üèóÔ∏è Section 1: Project Setup & Architecture PlanningBefore you start coding, you need to plan your system architecture and set up your development environment.### üéØ Section Objectives- Set up a professional development environment- Design your system architecture- Plan your data pipeline- Establish your project structure### ü§î Reflection Questions1. **What are the main components your NewsBot 2.0 needs?**2. **How will data flow through your system?**3. **What external APIs or services might you need?**4. **How will you handle errors and edge cases?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnFyUxwD1Z-J"
      },
      "outputs": [],
      "source": [
        "# üì¶ Environment Setup and Imports# TODO: Import all the libraries you'll need for your NewsBot 2.0# Standard librariesimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom collections import defaultdict, Counterimport reimport jsonimport warningswarnings.filterwarnings('ignore')# TODO: Add NLP libraries# Hint: You'll need libraries for:# - Text preprocessing (nltk, spacy)# - Machine learning (sklearn)# - Deep learning (transformers, torch)# - Topic modeling (gensim)# - Visualization (plotly, wordcloud)# - Web scraping (requests, beautifulsoup)# TODO: Add your imports hereprint(\"‚úÖ Environment setup complete!\")print(\"üéØ Ready to build NewsBot 2.0!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lSc_doE1Z-M"
      },
      "source": [
        "### üèóÔ∏è System Architecture DesignYour NewsBot 2.0 should have a **modular architecture** where each component has a specific responsibility.**Think about these questions:**- How will you organize your code into modules?- What classes and functions will you need?- How will components communicate with each other?- Where will you store configuration and settings?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDq9Ajjq1Z-N"
      },
      "outputs": [],
      "source": [
        "# üèóÔ∏è Architecture Planning# TODO: Design your system architectureclass NewsBot2Config:    \"\"\"    Configuration management for NewsBot 2.0    TODO: Define all your system settings here    \"\"\"    def __init__(self):        # TODO: Add configuration parameters        # Hint: Consider settings for:        # - API keys and endpoints        # - Model parameters        # - File paths and directories        # - Processing limits and thresholds        passclass NewsBot2System:    \"\"\"    Main system orchestrator for NewsBot 2.0    TODO: This will be your main system class    \"\"\"    def __init__(self, config):        self.config = config        # TODO: Initialize all your system components        # Hint: You'll need components for:        # - Data processing        # - Classification        # - Topic modeling        # - Language models        # - Multilingual processing        # - Conversational interface            def analyze_article(self, article_text):        \"\"\"        TODO: Implement comprehensive article analysis        This should return all the insights your system can generate        \"\"\"        pass        def process_query(self, user_query):        \"\"\"        TODO: Handle natural language queries from users        \"\"\"        pass        def generate_insights(self, articles):        \"\"\"        TODO: Generate high-level insights from multiple articles        \"\"\"        pass# TODO: Initialize your system# config = NewsBot2Config()# newsbot = NewsBot2System(config)print(\"üèóÔ∏è System architecture planned!\")print(\"üí° Next: Start implementing individual components\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RveevlUR1Z-P"
      },
      "source": [
        "## üìä Section 2: Advanced Content Analysis EngineThis is where you'll implement the core NLP analysis capabilities that make your NewsBot intelligent.### üéØ Section Objectives- Build enhanced text classification with confidence scoring- Implement topic modeling for content discovery- Create sentiment analysis with temporal tracking- Develop entity relationship mapping### üîó Course Module Connections- **Module 7**: Enhanced multi-class classification- **Module 8**: Advanced named entity recognition- **Module 9**: Topic modeling and clustering- **Module 6**: Sentiment analysis evolution### ü§î Key Questions to Consider1. **How will you handle multiple categories per article?**2. **What topics are most important to discover automatically?**3. **How can you track sentiment changes over time?**4. **What entity relationships are most valuable to extract?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJA8DTSL1Z-Q"
      },
      "outputs": [],
      "source": [
        "# üìä Advanced Classification System# TODO: Build your enhanced classification systemclass AdvancedNewsClassifier:    \"\"\"    Enhanced news classification with confidence scoring and multi-label support    TODO: This should be much more sophisticated than your midterm classifier    \"\"\"        def __init__(self):        # TODO: Initialize your classification models        # Hint: Consider using:        # - Multiple algorithms (ensemble methods)        # - Pre-trained language models        # - Custom feature engineering        # - Confidence scoring mechanisms        pass        def train(self, X_train, y_train):        \"\"\"        TODO: Train your classification models                Questions to consider:        - Will you use traditional ML or deep learning?        - How will you handle class imbalance?        - What evaluation metrics are most important?        - How will you tune hyperparameters?        \"\"\"        pass        def predict_with_confidence(self, article_text):        \"\"\"        TODO: Predict category with confidence scores                Should return:        - Primary category        - Confidence score        - Alternative categories with their scores        - Reasoning/explanation if possible        \"\"\"        pass        def explain_prediction(self, article_text):        \"\"\"        TODO: Provide explanation for classification decision                Hint: Consider using:        - Feature importance        - Key phrases that influenced decision        - Similar articles in training data        \"\"\"        pass# TODO: Test your classifier# classifier = AdvancedNewsClassifier()print(\"üìä Advanced classification system ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U45BSra81Z-R"
      },
      "outputs": [],
      "source": [
        "# üîç Topic Modeling and Discovery# TODO: Implement topic modeling for content discoveryclass TopicDiscoveryEngine:    \"\"\"    Advanced topic modeling for discovering themes and trends    TODO: Implement sophisticated topic analysis    \"\"\"        def __init__(self, n_topics=10, method='lda'):        # TODO: Initialize topic modeling components        # Hint: Consider:        # - LDA vs NMF vs other methods        # - Dynamic topic modeling for trend analysis        # - Hierarchical topic structures        # - Topic coherence evaluation        pass        def fit_topics(self, documents):        \"\"\"        TODO: Discover topics in document collection                Questions to consider:        - How will you preprocess text for topic modeling?        - What's the optimal number of topics?        - How will you handle topic evolution over time?        - How will you evaluate topic quality?        \"\"\"        pass        def get_article_topics(self, article_text):        \"\"\"        TODO: Get topic distribution for a single article        \"\"\"        pass        def track_topic_trends(self, articles_with_dates):        \"\"\"        TODO: Analyze how topics change over time                This is a key differentiator for your NewsBot 2.0!        Consider:        - Topic emergence and decline        - Seasonal patterns        - Event-driven topic spikes        - Cross-topic relationships        \"\"\"        pass        def visualize_topics(self):        \"\"\"        TODO: Create interactive topic visualizations                Hint: Consider using:        - pyLDAvis for LDA visualization        - Network graphs for topic relationships        - Timeline plots for topic evolution        - Word clouds for topic representation        \"\"\"        pass# TODO: Test your topic modeling# topic_engine = TopicDiscoveryEngine()print(\"üîç Topic discovery engine ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAjGCkQF1Z-T"
      },
      "outputs": [],
      "source": [
        "# üé≠ Advanced Sentiment Analysis# TODO: Implement sentiment analysis with temporal trackingclass SentimentEvolutionTracker:    \"\"\"    Advanced sentiment analysis with temporal and contextual understanding    TODO: Build sophisticated sentiment tracking    \"\"\"        def __init__(self):        # TODO: Initialize sentiment analysis components        # Hint: Consider:        # - Multiple sentiment dimensions (emotion, subjectivity, etc.)        # - Domain-specific sentiment models        # - Aspect-based sentiment analysis        # - Temporal sentiment patterns        pass        def analyze_sentiment(self, article_text):        \"\"\"        TODO: Comprehensive sentiment analysis                Should return:        - Overall sentiment (positive/negative/neutral)        - Confidence score        - Emotional dimensions (joy, anger, fear, etc.)        - Aspect-based sentiments (if applicable)        - Key phrases driving sentiment        \"\"\"        pass        def track_sentiment_over_time(self, articles_with_dates):        \"\"\"        TODO: Analyze sentiment trends over time                This is crucial for understanding public opinion evolution!        Consider:        - Daily/weekly/monthly sentiment trends        - Event-driven sentiment changes        - Topic-specific sentiment evolution        - Comparative sentiment across sources        \"\"\"        pass        def detect_sentiment_anomalies(self, sentiment_timeline):        \"\"\"        TODO: Identify unusual sentiment patterns                This could help detect:        - Breaking news events        - Public opinion shifts        - Misinformation campaigns        - Crisis situations        \"\"\"        pass# TODO: Test your sentiment tracker# sentiment_tracker = SentimentEvolutionTracker()print(\"üé≠ Sentiment evolution tracker ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS5CNogI1Z-U"
      },
      "outputs": [],
      "source": [
        "# üï∏Ô∏è Entity Relationship Mapping# TODO: Implement advanced entity recognition and relationship mappingclass EntityRelationshipMapper:    \"\"\"    Advanced NER with relationship extraction and network analysis    TODO: Build sophisticated entity understanding    \"\"\"        def __init__(self):        # TODO: Initialize NER and relationship extraction components        # Hint: Consider:        # - Multiple NER models (spaCy, transformers, custom)        # - Relationship extraction techniques        # - Entity linking and disambiguation        # - Knowledge graph construction        pass        def extract_entities(self, article_text):        \"\"\"        TODO: Extract and classify entities                Should identify:        - People (with roles/titles)        - Organizations (with types)        - Locations (with hierarchies)        - Events (with dates/contexts)        - Products, technologies, etc.        \"\"\"        pass        def extract_relationships(self, article_text):        \"\"\"        TODO: Extract relationships between entities                Examples:        - \"CEO of\" (person -> organization)        - \"located in\" (organization -> location)        - \"acquired by\" (organization -> organization)        - \"attended\" (person -> event)        \"\"\"        pass        def build_knowledge_graph(self, articles):        \"\"\"        TODO: Build knowledge graph from multiple articles                This creates a network of entities and relationships        that can reveal:        - Key players in different domains        - Hidden connections between entities        - Influence networks        - Trending relationships        \"\"\"        pass        def find_entity_connections(self, entity1, entity2):        \"\"\"        TODO: Find connections between two entities                This could help answer questions like:        - \"How are Apple and Tesla connected?\"        - \"What's the relationship between Biden and climate change?\"        \"\"\"        pass# TODO: Test your entity mapper# entity_mapper = EntityRelationshipMapper()print(\"üï∏Ô∏è Entity relationship mapper ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UncwBRR51Z-W"
      },
      "source": [
        "## üß† Section 3: Language Understanding & GenerationThis section focuses on advanced language model integration for summarization, content enhancement, and semantic understanding.### üéØ Section Objectives- Implement intelligent text summarization- Build content enhancement and expansion capabilities- Create semantic search and similarity matching- Develop query understanding and expansion### üîó Course Module Connections- **Module 10**: Neural networks and language models- **Module 11**: Advanced text generation techniques- **Module 12**: Natural language understanding### ü§î Key Questions to Consider1. **What makes a good summary for different types of news?**2. **How can you enhance articles with relevant context?**3. **What semantic relationships are most valuable to capture?**4. **How will you handle ambiguous or complex queries?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_bvxl4B1Z-X"
      },
      "outputs": [],
      "source": [
        "# üìù Intelligent Text Summarization# TODO: Implement advanced summarization capabilitiesclass IntelligentSummarizer:    \"\"\"    Advanced text summarization with multiple strategies and quality control    TODO: Build sophisticated summarization system    \"\"\"        def __init__(self):        # TODO: Initialize summarization models        # Hint: Consider:        # - Extractive vs abstractive summarization        # - Pre-trained models (BART, T5, etc.)        # - Domain-specific fine-tuning        # - Multi-document summarization        # - Quality assessment metrics        pass        def summarize_article(self, article_text, summary_type='balanced'):        \"\"\"        TODO: Generate high-quality article summary                Parameters:        - summary_type: 'brief', 'balanced', 'detailed'                Should consider:        - Article length and complexity        - Key information preservation        - Readability and coherence        - Factual accuracy        \"\"\"        pass        def summarize_multiple_articles(self, articles, focus_topic=None):        \"\"\"        TODO: Create unified summary from multiple articles                This is particularly valuable for:        - Breaking news coverage        - Topic-based summaries        - Trend analysis        - Comparative reporting        \"\"\"        pass        def generate_headlines(self, article_text):        \"\"\"        TODO: Generate compelling headlines                Consider different styles:        - Informative headlines        - Engaging headlines        - SEO-optimized headlines        - Social media headlines        \"\"\"        pass        def assess_summary_quality(self, original_text, summary):        \"\"\"        TODO: Evaluate summary quality                Metrics to consider:        - ROUGE scores        - Factual consistency        - Readability scores        - Information coverage        \"\"\"        pass# TODO: Test your summarizer# summarizer = IntelligentSummarizer()print(\"üìù Intelligent summarizer ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r4dfLkD1Z-Y"
      },
      "outputs": [],
      "source": [
        "# üîç Semantic Search and Similarity# TODO: Implement semantic understanding and search capabilitiesclass SemanticSearchEngine:    \"\"\"    Advanced semantic search using embeddings and similarity matching    TODO: Build sophisticated semantic understanding    \"\"\"        def __init__(self):        # TODO: Initialize semantic search components        # Hint: Consider:        # - Pre-trained embeddings (Word2Vec, GloVe, BERT)        # - Sentence-level embeddings        # - Document-level embeddings        # - Vector databases for efficient search        # - Similarity metrics and thresholds        pass        def encode_documents(self, documents):        \"\"\"        TODO: Convert documents to semantic embeddings                This creates vector representations that capture meaning        beyond just keyword matching        \"\"\"        pass        def find_similar_articles(self, query_article, top_k=5):        \"\"\"        TODO: Find semantically similar articles                This should find articles that are:        - Topically related        - Contextually similar        - Complementary in information        \"\"\"        pass        def semantic_search(self, query_text, article_database):        \"\"\"        TODO: Search articles using natural language queries                Examples:        - \"Articles about climate change policy\"        - \"Technology companies facing regulation\"        - \"Economic impact of pandemic\"        \"\"\"        pass        def cluster_similar_content(self, articles):        \"\"\"        TODO: Group articles by semantic similarity                This can help:        - Organize large article collections        - Identify story clusters        - Detect duplicate or near-duplicate content        - Find complementary perspectives        \"\"\"        pass# TODO: Test your semantic search# search_engine = SemanticSearchEngine()print(\"üîç Semantic search engine ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5luHbDyt1Z-Z"
      },
      "outputs": [],
      "source": [
        "# üí° Content Enhancement and Insights# TODO: Implement content enhancement and automatic insight generationclass ContentEnhancer:    \"\"\"    Advanced content analysis and enhancement system    TODO: Build intelligent content augmentation    \"\"\"        def __init__(self):        # TODO: Initialize content enhancement components        # Hint: Consider:        # - Knowledge bases and external APIs        # - Fact-checking capabilities        # - Context enrichment        # - Trend analysis        # - Comparative analysis        pass        def enhance_article(self, article_text):        \"\"\"        TODO: Add valuable context and insights to articles                Enhancements might include:        - Background information on key entities        - Related historical events        - Statistical context        - Expert opinions or analysis        - Fact-checking results        \"\"\"        pass        def generate_insights(self, articles):        \"\"\"        TODO: Generate high-level insights from article collection                Insights might include:        - Emerging trends and patterns        - Contradictory information        - Missing perspectives        - Key stakeholders and their positions        - Potential implications or consequences        \"\"\"        pass        def detect_information_gaps(self, articles, topic):        \"\"\"        TODO: Identify what information is missing                This could help:        - Guide further research        - Identify biased coverage        - Suggest follow-up questions        - Highlight underreported angles        \"\"\"        pass        def cross_reference_facts(self, article_text):        \"\"\"        TODO: Verify facts against reliable sources                This is increasingly important for:        - Combating misinformation        - Ensuring accuracy        - Building trust        - Providing transparency        \"\"\"        pass# TODO: Test your content enhancer# enhancer = ContentEnhancer()print(\"üí° Content enhancer ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvK7A9y61Z-a"
      },
      "source": [
        "## üåç Section 4: Multilingual IntelligenceThis section focuses on handling multiple languages and cross-cultural analysis - a key differentiator for NewsBot 2.0.### üéØ Section Objectives- Implement automatic language detection- Build translation and cross-lingual analysis capabilities- Create cultural context understanding- Develop comparative analysis across languages### üîó Course Module Connections- **Module 11**: Machine translation and multilingual processing- **Module 8**: Cross-lingual named entity recognition- **Module 9**: Multilingual topic modeling### ü§î Key Questions to Consider1. **What languages are most important for your use case?**2. **How will you handle cultural nuances and context?**3. **What insights can you gain from cross-language comparison?**4. **How will you ensure translation quality and accuracy?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32JsLQBh1Z-b"
      },
      "outputs": [],
      "source": [
        "# üåê Language Detection and Processing# TODO: Implement multilingual capabilitiesclass MultilingualProcessor:    \"\"\"    Advanced multilingual processing with language detection and cultural context    TODO: Build sophisticated multilingual understanding    \"\"\"        def __init__(self):        # TODO: Initialize multilingual components        # Hint: Consider:        # - Language detection models        # - Translation services (Google, Azure, etc.)        # - Multilingual embeddings        # - Cultural context databases        # - Cross-lingual NER models        pass        def detect_language(self, text):        \"\"\"        TODO: Detect language with confidence scoring                Should handle:        - Multiple languages in same text        - Short text snippets        - Code-switching        - Confidence thresholds        \"\"\"        pass        def translate_text(self, text, target_language='en'):        \"\"\"        TODO: High-quality translation with quality assessment                Consider:        - Multiple translation services        - Quality scoring        - Context preservation        - Cultural adaptation        \"\"\"        pass        def analyze_cross_lingual(self, articles_by_language):        \"\"\"        TODO: Compare coverage and perspectives across languages                This could reveal:        - Different cultural perspectives        - Varying coverage depth        - Regional biases        - Information gaps        \"\"\"        pass        def extract_cultural_context(self, text, source_language):        \"\"\"        TODO: Identify cultural references and context                This helps understand:        - Cultural idioms and expressions        - Regional references        - Historical context        - Social and political nuances        \"\"\"        pass# TODO: Test your multilingual processor# multilingual = MultilingualProcessor()print(\"üåê Multilingual processor ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCgfupPo1Z-c"
      },
      "source": [
        "## üí¨ Section 5: Conversational InterfaceThis section focuses on building natural language query capabilities that make your NewsBot truly interactive.### üéØ Section Objectives- Build intent classification for user queries- Implement natural language query processing- Create context-aware conversation management- Develop helpful response generation### üîó Course Module Connections- **Module 12**: Conversational AI and natural language understanding- **Module 7**: Intent classification- **Module 8**: Entity extraction from queries### ü§î Key Questions to Consider1. **What types of questions will users ask your NewsBot?**2. **How will you handle ambiguous or complex queries?**3. **What context do you need to maintain across conversations?**4. **How will you make responses helpful and actionable?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0ThMUTF1Z-c"
      },
      "outputs": [],
      "source": [
        "# üéØ Intent Classification and Query Understanding# TODO: Implement conversational AI capabilitiesclass ConversationalInterface:    \"\"\"    Advanced conversational AI for natural language interaction with NewsBot    TODO: Build sophisticated query understanding and response generation    \"\"\"        def __init__(self, newsbot_system):        self.newsbot = newsbot_system        # TODO: Initialize conversational components        # Hint: Consider:        # - Intent classification models        # - Entity extraction from queries        # - Context management        # - Response templates        # - Conversation state tracking        pass        def classify_intent(self, user_query):        \"\"\"        TODO: Classify user intent from natural language query                Common intents might include:        - \"search\" - Find articles about X        - \"summarize\" - Summarize articles about Y        - \"analyze\" - Analyze sentiment/trends for Z        - \"compare\" - Compare coverage of A vs B        - \"explain\" - Explain entity relationships        \"\"\"        pass        def extract_query_entities(self, user_query):        \"\"\"        TODO: Extract entities and parameters from user queries                Examples:        - \"Show me positive tech news from this week\"          -> entities: sentiment=positive, category=tech, timeframe=week        - \"Compare Apple and Google coverage\"          -> entities: companies=[Apple, Google], task=compare        \"\"\"        pass        def process_query(self, user_query, conversation_context=None):        \"\"\"        TODO: Process natural language query and generate response                This is the main interface between users and your NewsBot!                Should handle:        - Intent classification        - Entity extraction        - Query execution        - Response generation        - Context management        \"\"\"        pass        def generate_response(self, query_results, intent, entities):        \"\"\"        TODO: Generate helpful, natural language responses                Responses should be:        - Informative and accurate        - Appropriately detailed        - Actionable when possible        - Conversational in tone        \"\"\"        pass        def handle_follow_up(self, follow_up_query, conversation_history):        \"\"\"        TODO: Handle follow-up questions with context awareness                Examples:        - User: \"Show me tech news\"        - Bot: [shows results]        - User: \"What about from last month?\" (needs context)        \"\"\"        pass# TODO: Test your conversational interface# conversation = ConversationalInterface(newsbot_system)print(\"üí¨ Conversational interface ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqG-q3HW1Z-d"
      },
      "source": [
        "## üîß Section 6: System Integration & TestingThis section focuses on bringing all your components together into a cohesive, working system.### üéØ Section Objectives- Integrate all components into unified system- Implement comprehensive testing strategies- Build error handling and robustness- Create performance monitoring and optimization### ü§î Key Questions to Consider1. **How will your components communicate efficiently?**2. **What could go wrong and how will you handle it?**3. **How will you test complex, integrated functionality?**4. **What performance bottlenecks might you encounter?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cKCWkqy1Z-d"
      },
      "outputs": [],
      "source": [
        "# üîß System Integration and Orchestration# TODO: Bring all your components togetherclass NewsBot2IntegratedSystem:    \"\"\"    Complete NewsBot 2.0 system with all components integrated    TODO: This is your final, complete system    \"\"\"        def __init__(self, config):        self.config = config                # TODO: Initialize all your components        # self.classifier = AdvancedNewsClassifier()        # self.topic_engine = TopicDiscoveryEngine()        # self.sentiment_tracker = SentimentEvolutionTracker()        # self.entity_mapper = EntityRelationshipMapper()        # self.summarizer = IntelligentSummarizer()        # self.search_engine = SemanticSearchEngine()        # self.enhancer = ContentEnhancer()        # self.multilingual = MultilingualProcessor()        # self.conversation = ConversationalInterface(self)                # TODO: Set up system state and caching        pass        def comprehensive_analysis(self, article_text):        \"\"\"        TODO: Perform complete analysis of a single article                This should orchestrate all your analysis components        and return a comprehensive analysis report        \"\"\"        analysis_results = {            'classification': None,  # TODO: Use your classifier            'sentiment': None,       # TODO: Use your sentiment tracker            'entities': None,        # TODO: Use your entity mapper            'topics': None,          # TODO: Use your topic engine            'summary': None,         # TODO: Use your summarizer            'enhancements': None,    # TODO: Use your enhancer            'language': None,        # TODO: Use your multilingual processor        }                # TODO: Implement the orchestration logic        return analysis_results        def batch_analysis(self, articles):        \"\"\"        TODO: Analyze multiple articles efficiently                Consider:        - Parallel processing where possible        - Progress tracking        - Error handling for individual articles        - Memory management for large batches        \"\"\"        pass        def query_interface(self, user_query):        \"\"\"        TODO: Handle user queries through conversational interface                This is the main entry point for user interactions        \"\"\"        pass        def generate_insights_report(self, articles, report_type='comprehensive'):        \"\"\"        TODO: Generate comprehensive insights report                Report types might include:        - 'summary' - High-level overview        - 'comprehensive' - Detailed analysis        - 'trends' - Focus on temporal patterns        - 'comparative' - Cross-source comparison        \"\"\"        pass# TODO: Initialize your complete system# config = NewsBot2Config()# newsbot2 = NewsBot2IntegratedSystem(config)print(\"üîß Integrated system ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M6nbzMA1Z-e"
      },
      "outputs": [],
      "source": [
        "# üß™ Testing and Validation Framework# TODO: Implement comprehensive testing for your systemclass NewsBot2TestSuite:    \"\"\"    Comprehensive testing framework for NewsBot 2.0    TODO: Build thorough testing capabilities    \"\"\"        def __init__(self, newsbot_system):        self.newsbot = newsbot_system            def test_individual_components(self):        \"\"\"        TODO: Test each component individually                Unit tests for:        - Classification accuracy        - Topic modeling coherence        - Sentiment analysis accuracy        - Entity extraction precision/recall        - Translation quality        - Response generation quality        \"\"\"        test_results = {}                # TODO: Implement component tests        # test_results['classification'] = self.test_classification()        # test_results['topic_modeling'] = self.test_topic_modeling()        # test_results['sentiment'] = self.test_sentiment_analysis()        # test_results['ner'] = self.test_entity_extraction()        # test_results['summarization'] = self.test_summarization()        # test_results['translation'] = self.test_translation()                return test_results        def test_integration(self):        \"\"\"        TODO: Test integrated system functionality                Integration tests for:        - End-to-end article processing        - Query handling and response generation        - Multi-component workflows        - Error propagation and handling        \"\"\"        pass        def test_performance(self):        \"\"\"        TODO: Test system performance and scalability                Performance tests for:        - Processing speed        - Memory usage        - Concurrent request handling        - Large dataset processing        \"\"\"        pass        def test_edge_cases(self):        \"\"\"        TODO: Test system robustness with edge cases                Edge cases might include:        - Very short or very long articles        - Non-English text        - Malformed input        - Network failures        - API rate limits        \"\"\"        pass# TODO: Set up your testing framework# test_suite = NewsBot2TestSuite(newsbot2)print(\"üß™ Testing framework ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJhAUHCx1Z-e"
      },
      "source": [
        "## üìà Section 7: Evaluation & DocumentationThis final section focuses on evaluating your system's performance and creating professional documentation.### üéØ Section Objectives- Evaluate system performance using appropriate metrics- Create comprehensive technical documentation- Develop user-friendly guides and tutorials- Prepare professional presentation materials### ü§î Key Questions to Consider1. **What metrics best demonstrate your system's value?**2. **How will you communicate technical concepts to non-technical stakeholders?**3. **What documentation will users need to succeed with your system?**4. **How will you showcase your system's unique capabilities?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsxgO-8G1Z-f"
      },
      "outputs": [],
      "source": [
        "# üìä System Evaluation and Metrics# TODO: Implement comprehensive evaluation frameworkclass NewsBot2Evaluator:    \"\"\"    Comprehensive evaluation framework for NewsBot 2.0    TODO: Build thorough evaluation capabilities    \"\"\"        def __init__(self, newsbot_system):        self.newsbot = newsbot_system            def evaluate_classification_performance(self, test_data):        \"\"\"        TODO: Evaluate classification accuracy and performance                Metrics to calculate:        - Accuracy, Precision, Recall, F1-score        - Confusion matrices        - Per-class performance        - Confidence calibration        \"\"\"        pass        def evaluate_topic_modeling_quality(self, documents):        \"\"\"        TODO: Evaluate topic modeling effectiveness                Metrics to consider:        - Topic coherence scores        - Topic diversity        - Human interpretability        - Stability across runs        \"\"\"        pass        def evaluate_summarization_quality(self, articles_and_summaries):        \"\"\"        TODO: Evaluate summarization effectiveness                Metrics to consider:        - ROUGE scores        - Factual consistency        - Readability scores        - Information coverage        \"\"\"        pass        def evaluate_user_experience(self, user_interactions):        \"\"\"        TODO: Evaluate conversational interface effectiveness                Metrics to consider:        - Query understanding accuracy        - Response relevance        - User satisfaction scores        - Task completion rates        \"\"\"        pass        def generate_evaluation_report(self):        \"\"\"        TODO: Generate comprehensive evaluation report                This should include:        - Performance metrics for all components        - Comparative analysis with baselines        - Strengths and limitations        - Recommendations for improvement        \"\"\"        pass# TODO: Set up your evaluation framework# evaluator = NewsBot2Evaluator(newsbot2)print(\"üìä Evaluation framework ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU3QXY8R1Z-f"
      },
      "source": [
        "## üéØ Final Implementation Checklist### ‚úÖ Core Requirements Checklist#### **üìä Advanced Content Analysis Engine**- [ ] Enhanced multi-class classification with confidence scoring- [ ] Topic modeling with LDA/NMF for content discovery- [ ] Sentiment analysis with temporal tracking- [ ] Entity relationship mapping and knowledge graph construction- [ ] Performance evaluation with appropriate metrics#### **üß† Language Understanding & Generation**- [ ] Intelligent text summarization (extractive and/or abstractive)- [ ] Content enhancement with contextual information- [ ] Semantic search using embeddings- [ ] Query understanding and expansion capabilities- [ ] Quality assessment for generated content#### **üåç Multilingual Intelligence**- [ ] Automatic language detection with confidence scoring- [ ] Translation integration with quality assessment- [ ] Cross-lingual analysis and comparison- [ ] Cultural context understanding- [ ] Multilingual entity recognition#### **üí¨ Conversational Interface**- [ ] Intent classification for user queries- [ ] Natural language query processing- [ ] Context-aware conversation management- [ ] Helpful response generation- [ ] Follow-up question handling#### **üîß System Integration**- [ ] All components integrated into unified system- [ ] Comprehensive error handling and robustness- [ ] Performance optimization and monitoring- [ ] Thorough testing framework- [ ] Professional code organization and documentation### üìö Documentation Requirements- [ ] **Technical Documentation**: Architecture, API reference, installation guide- [ ] **User Documentation**: User guide, tutorials, FAQ- [ ] **Business Documentation**: Executive summary, ROI analysis, use cases- [ ] **Code Documentation**: Comprehensive docstrings and comments### üéØ Success CriteriaYour NewsBot 2.0 should demonstrate:- **Technical Excellence**: Sophisticated NLP capabilities that go beyond basic implementations- **Integration Mastery**: Seamless combination of multiple NLP techniques- **User Experience**: Intuitive, helpful interaction through natural language- **Professional Quality**: Production-ready code with proper documentation- **Innovation**: Creative solutions and novel applications of NLP techniques---## üöÄ Ready to Build Your NewsBot 2.0!You now have a comprehensive roadmap for building an advanced news intelligence system. Remember:### üí° Implementation Tips- **Start with core functionality** and build incrementally- **Test each component** thoroughly before integration- **Document as you go** - don't leave it until the end- **Ask for help** when you encounter challenges- **Be creative** - this is your chance to showcase your NLP skills!### üéØ Focus on Value- **Think like a product manager** - what would users actually want?- **Consider real-world applications** - how would this be used professionally?- **Emphasize unique capabilities** - what makes your NewsBot special?- **Demonstrate business impact** - how does this create value?### üèÜ Make It Portfolio-WorthyThis project should be something you're proud to show potential employers. Make it:- **Technically impressive** with sophisticated NLP implementations- **Well-documented** with clear explanations and examples- **Professionally presented** with clean code and good organization- **Practically valuable** with real-world applications and benefits**Good luck building your NewsBot 2.0!** ü§ñ‚ú®"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}