{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N48aanOQGGlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gahu0tYjGIb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy pandas scikit-learn nltk tqdm joblib\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN5uDmER9lX9",
        "outputId": "aae00cc3-7ace-41f0-de9f-855bd6726861"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize column names\n",
        "df = df.rename(columns=lambda x: x.strip().lower())\n",
        "rename_map = {'text':'text','content':'text','headline':'text','article':'text',\n",
        "              'category':'category','label':'category','class':'category'}\n",
        "df = df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns})\n",
        "\n",
        "# Fix missing or malformed structure\n",
        "if 'text' not in df.columns:\n",
        "    df['text'] = df.iloc[:,0].astype(str)\n",
        "if 'category' not in df.columns:\n",
        "    df['category'] = 'unknown'\n",
        "\n",
        "# Drop empties\n",
        "df = df.dropna(subset=['text']).reset_index(drop=True)\n",
        "print(\"‚úÖ Cleaned columns:\", df.columns.tolist())\n",
        "print(df.head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKFbqhlk-2qg",
        "outputId": "1c5606c7-0b42-4136-85af-6b4e561ef2a2"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned columns: ['{', 'text', 'category', 'clean_text', 'lemmatized', 'sentiment_score']\n",
            "                  {              text category clean_text lemmatized  \\\n",
            "0        \"cells\": [        \"cells\": [  unknown      cells       cell   \n",
            "1                 {                 {  unknown                         \n",
            "2     \"metadata\": {     \"metadata\": {  unknown   metadata   metadata   \n",
            "\n",
            "   sentiment_score  \n",
            "0              0.0  \n",
            "1              0.0  \n",
            "2              0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, spacy\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "lemmas = []\n",
        "for doc in tqdm(nlp.pipe(df['clean_text'], batch_size=32), total=len(df)):\n",
        "    lemmas.append(\" \".join([t.lemma_ for t in doc if not t.is_punct and not t.is_space and t.text not in stop_words]))\n",
        "df['lemmatized'] = lemmas\n",
        "print(\"‚úÖ Lemmatization complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNDYpVPG_BtR",
        "outputId": "cd3ca064-d14b-4047-add8-7065ecb9e874"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [00:02<00:00, 229.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Lemmatization complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "import numpy as np\n",
        "\n",
        "tfidf = TfidfVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2), max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(df['lemmatized'])\n",
        "\n",
        "def syntactic_features(text):\n",
        "    doc = nlp(text)\n",
        "    pos_counts = Counter([t.pos_ for t in doc])\n",
        "    L = max(len(doc), 1)\n",
        "    feats = {f'pos_{p}': pos_counts[p]/L for p in pos_counts}\n",
        "    return feats\n",
        "\n",
        "syn_feats = [syntactic_features(t) for t in tqdm(df['clean_text'])]\n",
        "syn_df = pd.DataFrame(syn_feats).fillna(0)\n",
        "X = hstack([X_tfidf, csr_matrix(syn_df.values)], format='csr')\n",
        "print(\"‚úÖ Feature matrix built:\", X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxzJPIAA_JOf",
        "outputId": "1a63bfa8-89d2-4926-b34f-afb2b68a880f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [00:05<00:00, 89.88it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Feature matrix built: (470, 44)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['category'] = df['category'].astype(str)\n",
        "unique_classes = df['category'].nunique()\n",
        "\n",
        "if unique_classes < 2:\n",
        "    print(\"‚ö†Ô∏è Only one unique category found; skipping training.\")\n",
        "    model = None\n",
        "else:\n",
        "    y = le.fit_transform(df['category'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000, solver='saga', multi_class='multinomial', n_jobs=-1)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"‚úÖ Model trained successfully!\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRVwZ495_Tmy",
        "outputId": "ebfcd9ea-ee1d-41a6-a4c9-7d55aef010fd"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Only one unique category found; skipping training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "df['sentiment_score'] = df['clean_text'].apply(lambda t: sid.polarity_scores(t)['compound'])\n",
        "print(\"‚úÖ Sentiment analysis added.\")\n",
        "df[['text','category','sentiment_score']].head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "11qX45Rh_aCw",
        "outputId": "8d9c50a9-99e0-4412-9004-ea93c4869595"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Sentiment analysis added.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text category  sentiment_score\n",
              "0                                         \"cells\": [  unknown              0.0\n",
              "1                                                  {  unknown              0.0\n",
              "2                                      \"metadata\": {  unknown              0.0\n",
              "3                                     \"execution\": {  unknown              0.0\n",
              "4       \"shell.execute_reply\": \"2025-09-01T13:15:...  unknown              0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf8ce481-3c87-4f8d-a5f7-98643cd2a855\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>sentiment_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"cells\": [</td>\n",
              "      <td>unknown</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{</td>\n",
              "      <td>unknown</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"metadata\": {</td>\n",
              "      <td>unknown</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"execution\": {</td>\n",
              "      <td>unknown</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"shell.execute_reply\": \"2025-09-01T13:15:...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf8ce481-3c87-4f8d-a5f7-98643cd2a855')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cf8ce481-3c87-4f8d-a5f7-98643cd2a855 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cf8ce481-3c87-4f8d-a5f7-98643cd2a855');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-380538d4-7b6c-4b2c-84e3-061d64472619\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-380538d4-7b6c-4b2c-84e3-061d64472619')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-380538d4-7b6c-4b2c-84e3-061d64472619 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[['text','category','sentiment_score']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"  {\",\n          \"     \\\"shell.execute_reply\\\": \\\"2025-09-01T13:15:36.284670Z\\\"\",\n          \"   \\\"metadata\\\": {\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"unknown\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "if model is not None:\n",
        "    joblib.dump(model, 'newsbot_model.joblib')\n",
        "    joblib.dump(tfidf, 'newsbot_tfidf.joblib')\n",
        "    joblib.dump(le, 'newsbot_label_encoder.joblib')\n",
        "    print(\"‚úÖ Model, TF-IDF, and encoder saved.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Model not saved (no training performed).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfcnPxFF_c_v",
        "outputId": "7d07fbb1-c52c-43db-c186-e325e1b44bee"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Model not saved (no training performed).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is not None and hasattr(model, \"coef_\"):\n",
        "    feature_names = tfidf.get_feature_names_out()\n",
        "    coefs = model.coef_\n",
        "    if coefs.shape[0] == 1:\n",
        "        importance = np.abs(coefs[0])\n",
        "    else:\n",
        "        importance = np.mean(np.abs(coefs), axis=0)\n",
        "    sorted_idx = np.argsort(importance)[::-1]\n",
        "    top_features = pd.DataFrame({\n",
        "        \"Feature\": feature_names[sorted_idx][:20],\n",
        "        \"Importance\": importance[sorted_idx][:20]\n",
        "    })\n",
        "    print(\"\\nüî• Top informative features:\\n\", top_features)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping feature importance ‚Äî model not trained.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XSZVcDU_kI1",
        "outputId": "5c29d420-8ed6-46e7-e1e9-2f1ffab1c67a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Skipping feature importance ‚Äî model not trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def predict_article_category(text):\n",
        "    if model is None:\n",
        "        return \"‚ö†Ô∏è Model not trained.\"\n",
        "    cleaned = clean_text(text)\n",
        "    lem_doc = nlp(cleaned)\n",
        "    lemmas = \" \".join([t.lemma_ for t in lem_doc if not t.is_punct and not t.is_space])\n",
        "    X_t = tfidf.transform([lemmas])\n",
        "    syn = syntactic_features(cleaned)\n",
        "    syn_row = csr_matrix([[syn.get(c, 0) for c in syn_df.columns]])\n",
        "    X_comb = hstack([X_t, syn_row], format='csr')\n",
        "    pred = model.predict(X_comb)\n",
        "    return le.inverse_transform(pred)[0]\n",
        "\n",
        "# üîç Example\n",
        "print(predict_article_category(\"Government announces new economic policy to boost jobs.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CPeL9IX_vbr",
        "outputId": "830a0e6c-8e21-47a3-fd23-fa484f1bcea4"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Model not trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/your-username/your-username-portfolio.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0ihm07wQY8-",
        "outputId": "72fbcff7-7c11-4a9d-ee29-aee53c1ab165"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'your-username-portfolio'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjexMmUK1Z-C"
      },
      "source": [
        "# ü§ñ NewsBot 2.0 Final Project - Student Guidance Notebook## üéØ Your Mission: Build an Advanced NLP Intelligence SystemWelcome to your final project! This notebook will guide you through building NewsBot 2.0 - a sophisticated news analysis platform that demonstrates everything you've learned in this course.### üöÄ What You're BuildingYou're creating a **production-ready news intelligence system** that can:- **Analyze** news articles with advanced NLP techniques- **Discover** hidden topics and trends in large text collections- **Understand** multiple languages and cultural contexts  - **Converse** with users through natural language queries- **Generate** insights and summaries automatically### üìö Skills You'll DemonstrateThis project integrates **ALL course modules**:- **Modules 1-2**: Advanced text preprocessing and feature engineering- **Modules 3-4**: Enhanced classification and linguistic analysis- **Modules 5-6**: Syntax parsing and semantic understanding- **Modules 7-8**: Multi-class classification and entity recognition- **Module 9**: Topic modeling and unsupervised learning- **Module 10**: Neural networks and language models- **Module 11**: Machine translation and multilingual processing- **Module 12**: Conversational AI and natural language understanding---## üó∫Ô∏è Project RoadmapThis notebook is organized into **7 major sections** that mirror your final system architecture:1. **üèóÔ∏è Project Setup & Architecture Planning**2. **üìä Advanced Content Analysis Engine** 3. **üß† Language Understanding & Generation**4. **üåç Multilingual Intelligence**5. **üí¨ Conversational Interface**6. **üîß System Integration & Testing**7. **üìà Evaluation & Documentation**Each section provides:- **Clear objectives** and success criteria- **Implementation hints** and architectural guidance- **Code templates** with TODO sections for you to complete- **Testing strategies** to validate your work- **Reflection questions** to deepen your understanding---## ‚ö†Ô∏è Important Notes### üéØ Learning Goals- **Understand** how advanced NLP systems work in production- **Implement** sophisticated text analysis pipelines- **Integrate** multiple NLP techniques into cohesive workflows- **Evaluate** system performance using appropriate metrics- **Communicate** technical concepts to business stakeholders### üö´ What This Notebook Won't Do- **Give you the answers** - you need to implement the logic- **Write your code** - you'll build everything from scratch- **Make decisions** - you'll choose the best approaches for your use case### ‚úÖ What This Notebook Will Do- **Guide your thinking** with structured questions and prompts- **Provide templates** and architectural patterns- **Suggest resources** and implementation strategies- **Help you organize** your work effectively- **Connect concepts** from different course modulesLet's begin building your NewsBot 2.0! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-baFClYAM0sE"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5KrNG8a1Z-H"
      },
      "source": [
        "## üèóÔ∏è Section 1: Project Setup & Architecture PlanningBefore you start coding, you need to plan your system architecture and set up your development environment.### üéØ Section Objectives- Set up a professional development environment- Design your system architecture- Plan your data pipeline- Establish your project structure### ü§î Reflection Questions1. **What are the main components your NewsBot 2.0 needs?**2. **How will data flow through your system?**3. **What external APIs or services might you need?**4. **How will you handle errors and edge cases?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "RnFyUxwD1Z-J"
      },
      "outputs": [],
      "source": [
        "# üì¶ Environment Setup and Imports# TODO: Import all the libraries you'll need for your NewsBot 2.0# Standard librariesimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom collections import defaultdict, Counterimport reimport jsonimport warningswarnings.filterwarnings('ignore')# TODO: Add NLP libraries# Hint: You'll need libraries for:# - Text preprocessing (nltk, spacy)# - Machine learning (sklearn)# - Deep learning (transformers, torch)# - Topic modeling (gensim)# - Visualization (plotly, wordcloud)# - Web scraping (requests, beautifulsoup)# TODO: Add your imports hereprint(\"‚úÖ Environment setup complete!\")print(\"üéØ Ready to build NewsBot 2.0!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lSc_doE1Z-M"
      },
      "source": [
        "### üèóÔ∏è System Architecture DesignYour NewsBot 2.0 should have a **modular architecture** where each component has a specific responsibility.**Think about these questions:**- How will you organize your code into modules?- What classes and functions will you need?- How will components communicate with each other?- Where will you store configuration and settings?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "ZDq9Ajjq1Z-N"
      },
      "outputs": [],
      "source": [
        "# üèóÔ∏è Architecture Planning# TODO: Design your system architectureclass NewsBot2Config:    \"\"\"    Configuration management for NewsBot 2.0    TODO: Define all your system settings here    \"\"\"    def __init__(self):        # TODO: Add configuration parameters        # Hint: Consider settings for:        # - API keys and endpoints        # - Model parameters        # - File paths and directories        # - Processing limits and thresholds        passclass NewsBot2System:    \"\"\"    Main system orchestrator for NewsBot 2.0    TODO: This will be your main system class    \"\"\"    def __init__(self, config):        self.config = config        # TODO: Initialize all your system components        # Hint: You'll need components for:        # - Data processing        # - Classification        # - Topic modeling        # - Language models        # - Multilingual processing        # - Conversational interface            def analyze_article(self, article_text):        \"\"\"        TODO: Implement comprehensive article analysis        This should return all the insights your system can generate        \"\"\"        pass        def process_query(self, user_query):        \"\"\"        TODO: Handle natural language queries from users        \"\"\"        pass        def generate_insights(self, articles):        \"\"\"        TODO: Generate high-level insights from multiple articles        \"\"\"        pass# TODO: Initialize your system# config = NewsBot2Config()# newsbot = NewsBot2System(config)print(\"üèóÔ∏è System architecture planned!\")print(\"üí° Next: Start implementing individual components\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RveevlUR1Z-P"
      },
      "source": [
        "## üìä Section 2: Advanced Content Analysis EngineThis is where you'll implement the core NLP analysis capabilities that make your NewsBot intelligent.### üéØ Section Objectives- Build enhanced text classification with confidence scoring- Implement topic modeling for content discovery- Create sentiment analysis with temporal tracking- Develop entity relationship mapping### üîó Course Module Connections- **Module 7**: Enhanced multi-class classification- **Module 8**: Advanced named entity recognition- **Module 9**: Topic modeling and clustering- **Module 6**: Sentiment analysis evolution### ü§î Key Questions to Consider1. **How will you handle multiple categories per article?**2. **What topics are most important to discover automatically?**3. **How can you track sentiment changes over time?**4. **What entity relationships are most valuable to extract?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "cJA8DTSL1Z-Q"
      },
      "outputs": [],
      "source": [
        "# üìä Advanced Classification System# TODO: Build your enhanced classification systemclass AdvancedNewsClassifier:    \"\"\"    Enhanced news classification with confidence scoring and multi-label support    TODO: This should be much more sophisticated than your midterm classifier    \"\"\"        def __init__(self):        # TODO: Initialize your classification models        # Hint: Consider using:        # - Multiple algorithms (ensemble methods)        # - Pre-trained language models        # - Custom feature engineering        # - Confidence scoring mechanisms        pass        def train(self, X_train, y_train):        \"\"\"        TODO: Train your classification models                Questions to consider:        - Will you use traditional ML or deep learning?        - How will you handle class imbalance?        - What evaluation metrics are most important?        - How will you tune hyperparameters?        \"\"\"        pass        def predict_with_confidence(self, article_text):        \"\"\"        TODO: Predict category with confidence scores                Should return:        - Primary category        - Confidence score        - Alternative categories with their scores        - Reasoning/explanation if possible        \"\"\"        pass        def explain_prediction(self, article_text):        \"\"\"        TODO: Provide explanation for classification decision                Hint: Consider using:        - Feature importance        - Key phrases that influenced decision        - Similar articles in training data        \"\"\"        pass# TODO: Test your classifier# classifier = AdvancedNewsClassifier()print(\"üìä Advanced classification system ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "U45BSra81Z-R"
      },
      "outputs": [],
      "source": [
        "# üîç Topic Modeling and Discovery# TODO: Implement topic modeling for content discoveryclass TopicDiscoveryEngine:    \"\"\"    Advanced topic modeling for discovering themes and trends    TODO: Implement sophisticated topic analysis    \"\"\"        def __init__(self, n_topics=10, method='lda'):        # TODO: Initialize topic modeling components        # Hint: Consider:        # - LDA vs NMF vs other methods        # - Dynamic topic modeling for trend analysis        # - Hierarchical topic structures        # - Topic coherence evaluation        pass        def fit_topics(self, documents):        \"\"\"        TODO: Discover topics in document collection                Questions to consider:        - How will you preprocess text for topic modeling?        - What's the optimal number of topics?        - How will you handle topic evolution over time?        - How will you evaluate topic quality?        \"\"\"        pass        def get_article_topics(self, article_text):        \"\"\"        TODO: Get topic distribution for a single article        \"\"\"        pass        def track_topic_trends(self, articles_with_dates):        \"\"\"        TODO: Analyze how topics change over time                This is a key differentiator for your NewsBot 2.0!        Consider:        - Topic emergence and decline        - Seasonal patterns        - Event-driven topic spikes        - Cross-topic relationships        \"\"\"        pass        def visualize_topics(self):        \"\"\"        TODO: Create interactive topic visualizations                Hint: Consider using:        - pyLDAvis for LDA visualization        - Network graphs for topic relationships        - Timeline plots for topic evolution        - Word clouds for topic representation        \"\"\"        pass# TODO: Test your topic modeling# topic_engine = TopicDiscoveryEngine()print(\"üîç Topic discovery engine ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "cAjGCkQF1Z-T"
      },
      "outputs": [],
      "source": [
        "# üé≠ Advanced Sentiment Analysis# TODO: Implement sentiment analysis with temporal trackingclass SentimentEvolutionTracker:    \"\"\"    Advanced sentiment analysis with temporal and contextual understanding    TODO: Build sophisticated sentiment tracking    \"\"\"        def __init__(self):        # TODO: Initialize sentiment analysis components        # Hint: Consider:        # - Multiple sentiment dimensions (emotion, subjectivity, etc.)        # - Domain-specific sentiment models        # - Aspect-based sentiment analysis        # - Temporal sentiment patterns        pass        def analyze_sentiment(self, article_text):        \"\"\"        TODO: Comprehensive sentiment analysis                Should return:        - Overall sentiment (positive/negative/neutral)        - Confidence score        - Emotional dimensions (joy, anger, fear, etc.)        - Aspect-based sentiments (if applicable)        - Key phrases driving sentiment        \"\"\"        pass        def track_sentiment_over_time(self, articles_with_dates):        \"\"\"        TODO: Analyze sentiment trends over time                This is crucial for understanding public opinion evolution!        Consider:        - Daily/weekly/monthly sentiment trends        - Event-driven sentiment changes        - Topic-specific sentiment evolution        - Comparative sentiment across sources        \"\"\"        pass        def detect_sentiment_anomalies(self, sentiment_timeline):        \"\"\"        TODO: Identify unusual sentiment patterns                This could help detect:        - Breaking news events        - Public opinion shifts        - Misinformation campaigns        - Crisis situations        \"\"\"        pass# TODO: Test your sentiment tracker# sentiment_tracker = SentimentEvolutionTracker()print(\"üé≠ Sentiment evolution tracker ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "kS5CNogI1Z-U"
      },
      "outputs": [],
      "source": [
        "# üï∏Ô∏è Entity Relationship Mapping# TODO: Implement advanced entity recognition and relationship mappingclass EntityRelationshipMapper:    \"\"\"    Advanced NER with relationship extraction and network analysis    TODO: Build sophisticated entity understanding    \"\"\"        def __init__(self):        # TODO: Initialize NER and relationship extraction components        # Hint: Consider:        # - Multiple NER models (spaCy, transformers, custom)        # - Relationship extraction techniques        # - Entity linking and disambiguation        # - Knowledge graph construction        pass        def extract_entities(self, article_text):        \"\"\"        TODO: Extract and classify entities                Should identify:        - People (with roles/titles)        - Organizations (with types)        - Locations (with hierarchies)        - Events (with dates/contexts)        - Products, technologies, etc.        \"\"\"        pass        def extract_relationships(self, article_text):        \"\"\"        TODO: Extract relationships between entities                Examples:        - \"CEO of\" (person -> organization)        - \"located in\" (organization -> location)        - \"acquired by\" (organization -> organization)        - \"attended\" (person -> event)        \"\"\"        pass        def build_knowledge_graph(self, articles):        \"\"\"        TODO: Build knowledge graph from multiple articles                This creates a network of entities and relationships        that can reveal:        - Key players in different domains        - Hidden connections between entities        - Influence networks        - Trending relationships        \"\"\"        pass        def find_entity_connections(self, entity1, entity2):        \"\"\"        TODO: Find connections between two entities                This could help answer questions like:        - \"How are Apple and Tesla connected?\"        - \"What's the relationship between Biden and climate change?\"        \"\"\"        pass# TODO: Test your entity mapper# entity_mapper = EntityRelationshipMapper()print(\"üï∏Ô∏è Entity relationship mapper ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UncwBRR51Z-W"
      },
      "source": [
        "## üß† Section 3: Language Understanding & GenerationThis section focuses on advanced language model integration for summarization, content enhancement, and semantic understanding.### üéØ Section Objectives- Implement intelligent text summarization- Build content enhancement and expansion capabilities- Create semantic search and similarity matching- Develop query understanding and expansion### üîó Course Module Connections- **Module 10**: Neural networks and language models- **Module 11**: Advanced text generation techniques- **Module 12**: Natural language understanding### ü§î Key Questions to Consider1. **What makes a good summary for different types of news?**2. **How can you enhance articles with relevant context?**3. **What semantic relationships are most valuable to capture?**4. **How will you handle ambiguous or complex queries?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "r_bvxl4B1Z-X"
      },
      "outputs": [],
      "source": [
        "# üìù Intelligent Text Summarization# TODO: Implement advanced summarization capabilitiesclass IntelligentSummarizer:    \"\"\"    Advanced text summarization with multiple strategies and quality control    TODO: Build sophisticated summarization system    \"\"\"        def __init__(self):        # TODO: Initialize summarization models        # Hint: Consider:        # - Extractive vs abstractive summarization        # - Pre-trained models (BART, T5, etc.)        # - Domain-specific fine-tuning        # - Multi-document summarization        # - Quality assessment metrics        pass        def summarize_article(self, article_text, summary_type='balanced'):        \"\"\"        TODO: Generate high-quality article summary                Parameters:        - summary_type: 'brief', 'balanced', 'detailed'                Should consider:        - Article length and complexity        - Key information preservation        - Readability and coherence        - Factual accuracy        \"\"\"        pass        def summarize_multiple_articles(self, articles, focus_topic=None):        \"\"\"        TODO: Create unified summary from multiple articles                This is particularly valuable for:        - Breaking news coverage        - Topic-based summaries        - Trend analysis        - Comparative reporting        \"\"\"        pass        def generate_headlines(self, article_text):        \"\"\"        TODO: Generate compelling headlines                Consider different styles:        - Informative headlines        - Engaging headlines        - SEO-optimized headlines        - Social media headlines        \"\"\"        pass        def assess_summary_quality(self, original_text, summary):        \"\"\"        TODO: Evaluate summary quality                Metrics to consider:        - ROUGE scores        - Factual consistency        - Readability scores        - Information coverage        \"\"\"        pass# TODO: Test your summarizer# summarizer = IntelligentSummarizer()print(\"üìù Intelligent summarizer ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "1r4dfLkD1Z-Y"
      },
      "outputs": [],
      "source": [
        "# üîç Semantic Search and Similarity# TODO: Implement semantic understanding and search capabilitiesclass SemanticSearchEngine:    \"\"\"    Advanced semantic search using embeddings and similarity matching    TODO: Build sophisticated semantic understanding    \"\"\"        def __init__(self):        # TODO: Initialize semantic search components        # Hint: Consider:        # - Pre-trained embeddings (Word2Vec, GloVe, BERT)        # - Sentence-level embeddings        # - Document-level embeddings        # - Vector databases for efficient search        # - Similarity metrics and thresholds        pass        def encode_documents(self, documents):        \"\"\"        TODO: Convert documents to semantic embeddings                This creates vector representations that capture meaning        beyond just keyword matching        \"\"\"        pass        def find_similar_articles(self, query_article, top_k=5):        \"\"\"        TODO: Find semantically similar articles                This should find articles that are:        - Topically related        - Contextually similar        - Complementary in information        \"\"\"        pass        def semantic_search(self, query_text, article_database):        \"\"\"        TODO: Search articles using natural language queries                Examples:        - \"Articles about climate change policy\"        - \"Technology companies facing regulation\"        - \"Economic impact of pandemic\"        \"\"\"        pass        def cluster_similar_content(self, articles):        \"\"\"        TODO: Group articles by semantic similarity                This can help:        - Organize large article collections        - Identify story clusters        - Detect duplicate or near-duplicate content        - Find complementary perspectives        \"\"\"        pass# TODO: Test your semantic search# search_engine = SemanticSearchEngine()print(\"üîç Semantic search engine ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "5luHbDyt1Z-Z"
      },
      "outputs": [],
      "source": [
        "# üí° Content Enhancement and Insights# TODO: Implement content enhancement and automatic insight generationclass ContentEnhancer:    \"\"\"    Advanced content analysis and enhancement system    TODO: Build intelligent content augmentation    \"\"\"        def __init__(self):        # TODO: Initialize content enhancement components        # Hint: Consider:        # - Knowledge bases and external APIs        # - Fact-checking capabilities        # - Context enrichment        # - Trend analysis        # - Comparative analysis        pass        def enhance_article(self, article_text):        \"\"\"        TODO: Add valuable context and insights to articles                Enhancements might include:        - Background information on key entities        - Related historical events        - Statistical context        - Expert opinions or analysis        - Fact-checking results        \"\"\"        pass        def generate_insights(self, articles):        \"\"\"        TODO: Generate high-level insights from article collection                Insights might include:        - Emerging trends and patterns        - Contradictory information        - Missing perspectives        - Key stakeholders and their positions        - Potential implications or consequences        \"\"\"        pass        def detect_information_gaps(self, articles, topic):        \"\"\"        TODO: Identify what information is missing                This could help:        - Guide further research        - Identify biased coverage        - Suggest follow-up questions        - Highlight underreported angles        \"\"\"        pass        def cross_reference_facts(self, article_text):        \"\"\"        TODO: Verify facts against reliable sources                This is increasingly important for:        - Combating misinformation        - Ensuring accuracy        - Building trust        - Providing transparency        \"\"\"        pass# TODO: Test your content enhancer# enhancer = ContentEnhancer()print(\"üí° Content enhancer ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvK7A9y61Z-a"
      },
      "source": [
        "## üåç Section 4: Multilingual IntelligenceThis section focuses on handling multiple languages and cross-cultural analysis - a key differentiator for NewsBot 2.0.### üéØ Section Objectives- Implement automatic language detection- Build translation and cross-lingual analysis capabilities- Create cultural context understanding- Develop comparative analysis across languages### üîó Course Module Connections- **Module 11**: Machine translation and multilingual processing- **Module 8**: Cross-lingual named entity recognition- **Module 9**: Multilingual topic modeling### ü§î Key Questions to Consider1. **What languages are most important for your use case?**2. **How will you handle cultural nuances and context?**3. **What insights can you gain from cross-language comparison?**4. **How will you ensure translation quality and accuracy?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "32JsLQBh1Z-b"
      },
      "outputs": [],
      "source": [
        "# üåê Language Detection and Processing# TODO: Implement multilingual capabilitiesclass MultilingualProcessor:    \"\"\"    Advanced multilingual processing with language detection and cultural context    TODO: Build sophisticated multilingual understanding    \"\"\"        def __init__(self):        # TODO: Initialize multilingual components        # Hint: Consider:        # - Language detection models        # - Translation services (Google, Azure, etc.)        # - Multilingual embeddings        # - Cultural context databases        # - Cross-lingual NER models        pass        def detect_language(self, text):        \"\"\"        TODO: Detect language with confidence scoring                Should handle:        - Multiple languages in same text        - Short text snippets        - Code-switching        - Confidence thresholds        \"\"\"        pass        def translate_text(self, text, target_language='en'):        \"\"\"        TODO: High-quality translation with quality assessment                Consider:        - Multiple translation services        - Quality scoring        - Context preservation        - Cultural adaptation        \"\"\"        pass        def analyze_cross_lingual(self, articles_by_language):        \"\"\"        TODO: Compare coverage and perspectives across languages                This could reveal:        - Different cultural perspectives        - Varying coverage depth        - Regional biases        - Information gaps        \"\"\"        pass        def extract_cultural_context(self, text, source_language):        \"\"\"        TODO: Identify cultural references and context                This helps understand:        - Cultural idioms and expressions        - Regional references        - Historical context        - Social and political nuances        \"\"\"        pass# TODO: Test your multilingual processor# multilingual = MultilingualProcessor()print(\"üåê Multilingual processor ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCgfupPo1Z-c"
      },
      "source": [
        "## üí¨ Section 5: Conversational InterfaceThis section focuses on building natural language query capabilities that make your NewsBot truly interactive.### üéØ Section Objectives- Build intent classification for user queries- Implement natural language query processing- Create context-aware conversation management- Develop helpful response generation### üîó Course Module Connections- **Module 12**: Conversational AI and natural language understanding- **Module 7**: Intent classification- **Module 8**: Entity extraction from queries### ü§î Key Questions to Consider1. **What types of questions will users ask your NewsBot?**2. **How will you handle ambiguous or complex queries?**3. **What context do you need to maintain across conversations?**4. **How will you make responses helpful and actionable?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "n0ThMUTF1Z-c"
      },
      "outputs": [],
      "source": [
        "# üéØ Intent Classification and Query Understanding# TODO: Implement conversational AI capabilitiesclass ConversationalInterface:    \"\"\"    Advanced conversational AI for natural language interaction with NewsBot    TODO: Build sophisticated query understanding and response generation    \"\"\"        def __init__(self, newsbot_system):        self.newsbot = newsbot_system        # TODO: Initialize conversational components        # Hint: Consider:        # - Intent classification models        # - Entity extraction from queries        # - Context management        # - Response templates        # - Conversation state tracking        pass        def classify_intent(self, user_query):        \"\"\"        TODO: Classify user intent from natural language query                Common intents might include:        - \"search\" - Find articles about X        - \"summarize\" - Summarize articles about Y        - \"analyze\" - Analyze sentiment/trends for Z        - \"compare\" - Compare coverage of A vs B        - \"explain\" - Explain entity relationships        \"\"\"        pass        def extract_query_entities(self, user_query):        \"\"\"        TODO: Extract entities and parameters from user queries                Examples:        - \"Show me positive tech news from this week\"          -> entities: sentiment=positive, category=tech, timeframe=week        - \"Compare Apple and Google coverage\"          -> entities: companies=[Apple, Google], task=compare        \"\"\"        pass        def process_query(self, user_query, conversation_context=None):        \"\"\"        TODO: Process natural language query and generate response                This is the main interface between users and your NewsBot!                Should handle:        - Intent classification        - Entity extraction        - Query execution        - Response generation        - Context management        \"\"\"        pass        def generate_response(self, query_results, intent, entities):        \"\"\"        TODO: Generate helpful, natural language responses                Responses should be:        - Informative and accurate        - Appropriately detailed        - Actionable when possible        - Conversational in tone        \"\"\"        pass        def handle_follow_up(self, follow_up_query, conversation_history):        \"\"\"        TODO: Handle follow-up questions with context awareness                Examples:        - User: \"Show me tech news\"        - Bot: [shows results]        - User: \"What about from last month?\" (needs context)        \"\"\"        pass# TODO: Test your conversational interface# conversation = ConversationalInterface(newsbot_system)print(\"üí¨ Conversational interface ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqG-q3HW1Z-d"
      },
      "source": [
        "## üîß Section 6: System Integration & TestingThis section focuses on bringing all your components together into a cohesive, working system.### üéØ Section Objectives- Integrate all components into unified system- Implement comprehensive testing strategies- Build error handling and robustness- Create performance monitoring and optimization### ü§î Key Questions to Consider1. **How will your components communicate efficiently?**2. **What could go wrong and how will you handle it?**3. **How will you test complex, integrated functionality?**4. **What performance bottlenecks might you encounter?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "2cKCWkqy1Z-d"
      },
      "outputs": [],
      "source": [
        "# üîß System Integration and Orchestration# TODO: Bring all your components togetherclass NewsBot2IntegratedSystem:    \"\"\"    Complete NewsBot 2.0 system with all components integrated    TODO: This is your final, complete system    \"\"\"        def __init__(self, config):        self.config = config                # TODO: Initialize all your components        # self.classifier = AdvancedNewsClassifier()        # self.topic_engine = TopicDiscoveryEngine()        # self.sentiment_tracker = SentimentEvolutionTracker()        # self.entity_mapper = EntityRelationshipMapper()        # self.summarizer = IntelligentSummarizer()        # self.search_engine = SemanticSearchEngine()        # self.enhancer = ContentEnhancer()        # self.multilingual = MultilingualProcessor()        # self.conversation = ConversationalInterface(self)                # TODO: Set up system state and caching        pass        def comprehensive_analysis(self, article_text):        \"\"\"        TODO: Perform complete analysis of a single article                This should orchestrate all your analysis components        and return a comprehensive analysis report        \"\"\"        analysis_results = {            'classification': None,  # TODO: Use your classifier            'sentiment': None,       # TODO: Use your sentiment tracker            'entities': None,        # TODO: Use your entity mapper            'topics': None,          # TODO: Use your topic engine            'summary': None,         # TODO: Use your summarizer            'enhancements': None,    # TODO: Use your enhancer            'language': None,        # TODO: Use your multilingual processor        }                # TODO: Implement the orchestration logic        return analysis_results        def batch_analysis(self, articles):        \"\"\"        TODO: Analyze multiple articles efficiently                Consider:        - Parallel processing where possible        - Progress tracking        - Error handling for individual articles        - Memory management for large batches        \"\"\"        pass        def query_interface(self, user_query):        \"\"\"        TODO: Handle user queries through conversational interface                This is the main entry point for user interactions        \"\"\"        pass        def generate_insights_report(self, articles, report_type='comprehensive'):        \"\"\"        TODO: Generate comprehensive insights report                Report types might include:        - 'summary' - High-level overview        - 'comprehensive' - Detailed analysis        - 'trends' - Focus on temporal patterns        - 'comparative' - Cross-source comparison        \"\"\"        pass# TODO: Initialize your complete system# config = NewsBot2Config()# newsbot2 = NewsBot2IntegratedSystem(config)print(\"üîß Integrated system ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "_M6nbzMA1Z-e"
      },
      "outputs": [],
      "source": [
        "# üß™ Testing and Validation Framework# TODO: Implement comprehensive testing for your systemclass NewsBot2TestSuite:    \"\"\"    Comprehensive testing framework for NewsBot 2.0    TODO: Build thorough testing capabilities    \"\"\"        def __init__(self, newsbot_system):        self.newsbot = newsbot_system            def test_individual_components(self):        \"\"\"        TODO: Test each component individually                Unit tests for:        - Classification accuracy        - Topic modeling coherence        - Sentiment analysis accuracy        - Entity extraction precision/recall        - Translation quality        - Response generation quality        \"\"\"        test_results = {}                # TODO: Implement component tests        # test_results['classification'] = self.test_classification()        # test_results['topic_modeling'] = self.test_topic_modeling()        # test_results['sentiment'] = self.test_sentiment_analysis()        # test_results['ner'] = self.test_entity_extraction()        # test_results['summarization'] = self.test_summarization()        # test_results['translation'] = self.test_translation()                return test_results        def test_integration(self):        \"\"\"        TODO: Test integrated system functionality                Integration tests for:        - End-to-end article processing        - Query handling and response generation        - Multi-component workflows        - Error propagation and handling        \"\"\"        pass        def test_performance(self):        \"\"\"        TODO: Test system performance and scalability                Performance tests for:        - Processing speed        - Memory usage        - Concurrent request handling        - Large dataset processing        \"\"\"        pass        def test_edge_cases(self):        \"\"\"        TODO: Test system robustness with edge cases                Edge cases might include:        - Very short or very long articles        - Non-English text        - Malformed input        - Network failures        - API rate limits        \"\"\"        pass# TODO: Set up your testing framework# test_suite = NewsBot2TestSuite(newsbot2)print(\"üß™ Testing framework ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJhAUHCx1Z-e"
      },
      "source": [
        "## üìà Section 7: Evaluation & DocumentationThis final section focuses on evaluating your system's performance and creating professional documentation.### üéØ Section Objectives- Evaluate system performance using appropriate metrics- Create comprehensive technical documentation- Develop user-friendly guides and tutorials- Prepare professional presentation materials### ü§î Key Questions to Consider1. **What metrics best demonstrate your system's value?**2. **How will you communicate technical concepts to non-technical stakeholders?**3. **What documentation will users need to succeed with your system?**4. **How will you showcase your system's unique capabilities?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "FsxgO-8G1Z-f"
      },
      "outputs": [],
      "source": [
        "# üìä System Evaluation and Metrics# TODO: Implement comprehensive evaluation frameworkclass NewsBot2Evaluator:    \"\"\"    Comprehensive evaluation framework for NewsBot 2.0    TODO: Build thorough evaluation capabilities    \"\"\"        def __init__(self, newsbot_system):        self.newsbot = newsbot_system            def evaluate_classification_performance(self, test_data):        \"\"\"        TODO: Evaluate classification accuracy and performance                Metrics to calculate:        - Accuracy, Precision, Recall, F1-score        - Confusion matrices        - Per-class performance        - Confidence calibration        \"\"\"        pass        def evaluate_topic_modeling_quality(self, documents):        \"\"\"        TODO: Evaluate topic modeling effectiveness                Metrics to consider:        - Topic coherence scores        - Topic diversity        - Human interpretability        - Stability across runs        \"\"\"        pass        def evaluate_summarization_quality(self, articles_and_summaries):        \"\"\"        TODO: Evaluate summarization effectiveness                Metrics to consider:        - ROUGE scores        - Factual consistency        - Readability scores        - Information coverage        \"\"\"        pass        def evaluate_user_experience(self, user_interactions):        \"\"\"        TODO: Evaluate conversational interface effectiveness                Metrics to consider:        - Query understanding accuracy        - Response relevance        - User satisfaction scores        - Task completion rates        \"\"\"        pass        def generate_evaluation_report(self):        \"\"\"        TODO: Generate comprehensive evaluation report                This should include:        - Performance metrics for all components        - Comparative analysis with baselines        - Strengths and limitations        - Recommendations for improvement        \"\"\"        pass# TODO: Set up your evaluation framework# evaluator = NewsBot2Evaluator(newsbot2)print(\"üìä Evaluation framework ready for implementation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU3QXY8R1Z-f"
      },
      "source": [
        "## üéØ Final Implementation Checklist### ‚úÖ Core Requirements Checklist#### **üìä Advanced Content Analysis Engine**- [ ] Enhanced multi-class classification with confidence scoring- [ ] Topic modeling with LDA/NMF for content discovery- [ ] Sentiment analysis with temporal tracking- [ ] Entity relationship mapping and knowledge graph construction- [ ] Performance evaluation with appropriate metrics#### **üß† Language Understanding & Generation**- [ ] Intelligent text summarization (extractive and/or abstractive)- [ ] Content enhancement with contextual information- [ ] Semantic search using embeddings- [ ] Query understanding and expansion capabilities- [ ] Quality assessment for generated content#### **üåç Multilingual Intelligence**- [ ] Automatic language detection with confidence scoring- [ ] Translation integration with quality assessment- [ ] Cross-lingual analysis and comparison- [ ] Cultural context understanding- [ ] Multilingual entity recognition#### **üí¨ Conversational Interface**- [ ] Intent classification for user queries- [ ] Natural language query processing- [ ] Context-aware conversation management- [ ] Helpful response generation- [ ] Follow-up question handling#### **üîß System Integration**- [ ] All components integrated into unified system- [ ] Comprehensive error handling and robustness- [ ] Performance optimization and monitoring- [ ] Thorough testing framework- [ ] Professional code organization and documentation### üìö Documentation Requirements- [ ] **Technical Documentation**: Architecture, API reference, installation guide- [ ] **User Documentation**: User guide, tutorials, FAQ- [ ] **Business Documentation**: Executive summary, ROI analysis, use cases- [ ] **Code Documentation**: Comprehensive docstrings and comments### üéØ Success CriteriaYour NewsBot 2.0 should demonstrate:- **Technical Excellence**: Sophisticated NLP capabilities that go beyond basic implementations- **Integration Mastery**: Seamless combination of multiple NLP techniques- **User Experience**: Intuitive, helpful interaction through natural language- **Professional Quality**: Production-ready code with proper documentation- **Innovation**: Creative solutions and novel applications of NLP techniques---## üöÄ Ready to Build Your NewsBot 2.0!You now have a comprehensive roadmap for building an advanced news intelligence system. Remember:### üí° Implementation Tips- **Start with core functionality** and build incrementally- **Test each component** thoroughly before integration- **Document as you go** - don't leave it until the end- **Ask for help** when you encounter challenges- **Be creative** - this is your chance to showcase your NLP skills!### üéØ Focus on Value- **Think like a product manager** - what would users actually want?- **Consider real-world applications** - how would this be used professionally?- **Emphasize unique capabilities** - what makes your NewsBot special?- **Demonstrate business impact** - how does this create value?### üèÜ Make It Portfolio-WorthyThis project should be something you're proud to show potential employers. Make it:- **Technically impressive** with sophisticated NLP implementations- **Well-documented** with clear explanations and examples- **Professionally presented** with clean code and good organization- **Practically valuable** with real-world applications and benefits**Good luck building your NewsBot 2.0!** ü§ñ‚ú®"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}